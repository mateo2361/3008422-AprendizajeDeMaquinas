{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller Reducción de la dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curso Aprendizaje de Maquina \n",
    "John Willian Branch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Extracción manual de características\n",
    "Se desea comparar precios de productos específicos entre tiendas. Las características del conjunto de datos pre-cargado sales_df son: storeID, product, quantity y revenue. Las características quantity y  revenue indican cuántos artículos de un producto en particular se vendieron en una tienda y cuáles fueron los ingresos totales. Para el propósito de su análisis, es más interesante saber el precio promedio por producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sales_df=pd.read_csv(\"lab4-1.csv\")#reading a dataset in a dataframe using pandas\n",
    "print(\"sales_df\",sales_df.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Calcule el precio del producto a partir de la cantidad vendida (quantity) y los ingresos totales (revenue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the price from the quantity sold and revenue\n",
    "sales_df['price'] = ____\n",
    "print(\"sales_df\",sales_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarte las características cantidad vendida (quantity) y ingresos totales (revenue) del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the quantity and revenue features\n",
    "reduced_df = sales_df.drop(____, axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracción manual de características II\n",
    "Estás trabajando en una variante del conjunto de datos ANSUR height_df, donde la altura de una persona se midió 3 veces. Agregue una característica con la altura media al conjunto de datos y luego suelte las 3 características originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_df=pd.read_csv(\"lab4-2.csv\")#reading a dataset in a dataframe using pandas\n",
    "print(\"sales_df\",height_df.shape)\n",
    "print(\"sales_df\",height_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregue una característica con la altura media al conjunto de datos. Usa el .mean()método con axis=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean height\n",
    "height_df['height'] = height_df[____].____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarte las 3 características de altura originales del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 3 original height features\n",
    "reduced_df = height_df.drop(____, axis=1)\n",
    "print(\"reduced_df\",reduced_df.shape)\n",
    "print(\"reduced_df\",reduced_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Cálculo de Componentes Principales (PCA)\n",
    "Inspeccionará visualmente una muestra de 4 características del conjunto de datos ANSUR antes y después de aplicar  PCA usando Seaborn's pairplot(). Esto le permitirá inspeccionar las correlaciones por pares entre las características.\n",
    "\n",
    "Los datos han sido precargados para usted como ansur_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansur_df=pd.read_csv(\"Ansur_PCA_1.csv\")#reading a dataset in a dataframe using pandas\n",
    "print(\"ansur_df\",ansur_df.shape)\n",
    "print(\"ansur_df\",ansur_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un pairplot de Seaborn para inspeccionar ansur_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot to inspect ansur_df\n",
    "import seaborn as sns\n",
    "sns.____(____)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un mecanismo de escalamiento y estandarizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create the scaler and standardize the data\n",
    "scaler = ____\n",
    "ansur_std = scaler.____\n",
    "print(ansur_std[1:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una instancia PCA() y ajustamos y transformamos los datos estandarizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = ____\n",
    "pc = pca.____\n",
    "\n",
    "# This changes the numpy array output back to a dataframe\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "print(pc_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un pairplot para graficar el dataframe de componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot of the principal component dataframe\n",
    "sns.____(____)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA en un conjunto de datos más grande\n",
    "Ahora aplicará PCA en una muestra de datos ANSUR algo más grande con 13 dimensiones, una vez más precargada como ansur_df. El modelo ajustado se usará en el próximo ejercicio. Dado que no estamos utilizando los componentes principales en sí mismos, no es necesario transformar los datos, en cambio, es suficiente para adaptarse pcaa los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansur_df=pd.read_csv(\"Ansur_PCA_larger.csv\")#reading a dataset in a dataframe using pandas\n",
    "print(\"ansur_df\",ansur_df.shape)\n",
    "print(\"ansur_df\",ansur_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea el scaler.\n",
    "Estandarizar los datos.\n",
    "Crea la instancia  PCA().\n",
    "Ajústelo a los datos estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "scaler = ____\n",
    "ansur_std = ____\n",
    "\n",
    "# Apply PCA\n",
    "pca = ____\n",
    "pca.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Excelente! Ha ajustadp PCA en nuestra muestra de datos de 13 dimensiones. Ahora veamos cómo los componentes explican la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA explica la varianza\n",
    "Inspeccionará la varianza explicada por los diferentes componentes principales de la instancia pca() que creó en el ejercicio anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprima la razón de varianza por componente principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the explained variance ratio per component\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta\n",
    "¿Qué parte de la varianza se explica por el cuarto componente principal?<br>\n",
    "    Respuestas posibles:<br>\n",
    "    • Alrededor de 3.03%<br>\n",
    "    • Alrededor de 3.77%<br>\n",
    "    • Alrededor de 6.8%<br>\n",
    "    • Sobre 61.45%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprima la suma acumulativa de la razón de varianza explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es el número más bajo de componentes principales que debe mantener si no desea perder más del 10% de la varianza explicada durante la reducción de dimensionalidad?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Comprendiendo los componentes\n",
    "Ahora se aplicará PCA a las características numéricas del conjunto de datos de Pokemon poke_df, utilizando un pipe line para combinar la escala de características y PCA en un sólo comando. Luego interpretará los significados de los dos primeros componentes.\n",
    "\n",
    "Todos los paquetes y las clases pertinentes han sido pre-cargado para usted ( Pipeline(), StandardScaler(), PCA())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "poke_df=pd.read_csv(\"Pokemon-6dim.csv\")#reading a dataset in a dataframe using pandas\n",
    "print(\"poke_df\",poke_df.shape)\n",
    "print(\"poke_df\",poke_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construya el pipeline con la función de escalamiento y una instancia PCA, establezca el número de componentes para calcular en 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', ____),\n",
    "        \t\t ('reducer', ____)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste el pipeline al dataset y extraiga los vectores componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta:<br>\n",
    "Inspeccione el PC 1. Cual de los siguientes es verdadero:<br>\n",
    "    A. La velocidad de ataque (Attack Speed) tiene el mayor efecto en esta característica con diferencia. La PC 1 se puede interpretar como una medida de qué tan rápido puede atacar un Pokémon.<br>\n",
    "    B. Todas las características tienen un efecto positivo similar. La PC 1 puede interpretarse como una medida de la calidad general (estadísticas altas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta:<br>\n",
    "Inspeccione el PC 2. Cual de los siguientes es verdadero:<br>\n",
    "    A. La defensa (Defense) tiene un fuerte efecto positivo en el segundo componente y la veloscidad (speed) una influencia  fuerte negativa. Este componente cuantifica un equilibrio entre agilidad vs. armadura y protección (agility vs. armor & protection).<br>\n",
    "    B. Todas las funciones relacionadas con la velocidad tienen un efecto negativo en este componente. Los Pokémon rápidos tienen valores altos para este componente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA para la exploración de características\n",
    "Ahora Utilizará el pipeline  de PCA que ha creado en el ejercicio anterior para explorar visualmente cómo algunas características categóricas se relacionan con la variación poke_df. Estas características categóricas ( Type& Legendary) se pueden encontrar en un marco de datos separado poke_cat_df.\n",
    "\n",
    "Todos los paquetes y las clases pertinentes han sido pre-cargado para usted ( Pipeline(), StandardScaler(), PCA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poke_cat_df=pd.read_csv(\"Pokemon_cat800.csv\")#reading a dataset in a dataframe using pandas\n",
    "print(\"poke_df\",poke_cat_df.shape)\n",
    "print(\"poke_df\",poke_cat_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste y transforme el pipeline poke_df para extraer los componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = _______\n",
    "\n",
    "print(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregue los 2 componentes a poke_cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = ____\n",
    "poke_cat_df['PC 2'] = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use la función Tipo para colorear el diagrama de dispersión PC 1 vs PC 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=____, \n",
    "                x=____, y=____, hue=____)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilice la caracyteristica \"legendary\" para colorear el diagrama de dispersión PC 1 vs PC 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Legendary feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue=____)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PCA en un modelo Pipeline\n",
    "Acabamos de ver que los Pokémon legendarios tienden a tener estadísticas más altas en general. Veamos si podemos agregar un clasificador a nuestro Pipeline que detecte Pokémon legendarios versus no legendarios en función de los componentes principales.\n",
    "\n",
    "Los datos han sido pre-cargado para usted y dividido en conjuntos de datos de entrenamiento y prueba: X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=pd.read_csv(\"Pokemon-6dim.csv\")#reading a dataset in a dataframe using pandas\n",
    "y=pd.read_csv(\"Pokemon-6dimY.csv\")#reading a dataset in a dataframe using pandas\n",
    "y=y['yValue']\n",
    "print(\"X\",X.shape)\n",
    "print(\"y\",y.shape)\n",
    "# Perform a 70% train and 30% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregue un escalador, PCA limitado a 2 componentes y un clasificador de bosque aleatorio con random_state=0 al pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', ____),\n",
    "        ('reducer', ____),\n",
    "        ('classifier', ____)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste el pipeline a los datos de entrenamiento. E imprima la relación de varianza explicada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "____\n",
    "\n",
    "# Prints the explained variance ratio\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúe la precisión en el conjunto de pruebas, e imprima la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.____\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repita el proceso , pero esta vez con 3 componentes PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=____)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Seleccionar la proporción de varianza a mantener\n",
    "\n",
    "Ahorá usted dejará que PCA determine la cantidad de componentes a calcular en función de un umbral de varianza explicado que usted decida.\n",
    "\n",
    "Trabajará en el conjunto de datos numéricos ANSUR hembra precargado como ansur_df.\n",
    "\n",
    "Todos los paquetes y las clases pertinentes han sido pre-cargado también ( Pipeline(), StandardScaler(), PCA())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansur_df=pd.read_csv(\"ANSUR_II_FEMALE.csv\")#reading a dataset in a dataframe using pandas\n",
    "#Create a mask with the columns that we are going to keep\n",
    "mask=['abdominalextensiondepthsitting',\n",
    " 'acromialheight',\n",
    " 'acromionradialelength',\n",
    " 'anklecircumference',\n",
    " 'axillaheight',\n",
    " 'balloffootcircumference',\n",
    " 'balloffootlength',\n",
    " 'biacromialbreadth',\n",
    " 'bicepscircumferenceflexed',\n",
    " 'bicristalbreadth',\n",
    " 'bideltoidbreadth',\n",
    " 'bimalleolarbreadth',\n",
    " 'bitragionchinarc',\n",
    " 'bitragionsubmandibulararc',\n",
    " 'bizygomaticbreadth',\n",
    " 'buttockcircumference',\n",
    " 'buttockdepth',\n",
    " 'buttockheight',\n",
    " 'buttockkneelength',\n",
    " 'buttockpopliteallength',\n",
    " 'calfcircumference',\n",
    " 'cervicaleheight',\n",
    " 'chestbreadth',\n",
    " 'chestcircumference',\n",
    " 'chestdepth',\n",
    " 'chestheight',\n",
    " 'crotchheight',\n",
    " 'crotchlengthomphalion',\n",
    " 'crotchlengthposterioromphalion',\n",
    " 'earbreadth',\n",
    " 'earlength',\n",
    " 'earprotrusion',\n",
    " 'elbowrestheight',\n",
    " 'eyeheightsitting',\n",
    " 'footbreadthhorizontal',\n",
    " 'footlength',\n",
    " 'forearmcenterofgriplength',\n",
    " 'forearmcircumferenceflexed',\n",
    " 'forearmforearmbreadth',\n",
    " 'forearmhandlength',\n",
    " 'functionalleglength',\n",
    " 'handbreadth',\n",
    " 'handcircumference',\n",
    " 'handlength',\n",
    " 'headbreadth',\n",
    " 'headcircumference',\n",
    " 'headlength',\n",
    " 'heelanklecircumference',\n",
    " 'heelbreadth',\n",
    " 'hipbreadth',\n",
    " 'hipbreadthsitting',\n",
    " 'iliocristaleheight',\n",
    " 'interpupillarybreadth',\n",
    " 'interscyei',\n",
    " 'interscyeii',\n",
    " 'kneeheightmidpatella',\n",
    " 'kneeheightsitting',\n",
    " 'lateralfemoralepicondyleheight',\n",
    " 'lateralmalleolusheight',\n",
    " 'lowerthighcircumference',\n",
    " 'mentonsellionlength',\n",
    " 'neckcircumference',\n",
    " 'neckcircumferencebase',\n",
    " 'overheadfingertipreachsitting',\n",
    " 'palmlength',\n",
    " 'poplitealheight',\n",
    " 'radialestylionlength',\n",
    " 'shouldercircumference',\n",
    " 'shoulderelbowlength',\n",
    " 'shoulderlength',\n",
    " 'sittingheight',\n",
    " 'sleevelengthspinewrist',\n",
    " 'sleeveoutseam',\n",
    " 'span',\n",
    " 'suprasternaleheight',\n",
    " 'tenthribheight',\n",
    " 'thighcircumference',\n",
    " 'thighclearance',\n",
    " 'thumbtipreach',\n",
    " 'tibialheight',\n",
    " 'tragiontopofhead',\n",
    " 'trochanterionheight',\n",
    " 'verticaltrunkcircumferenceusa',\n",
    " 'waistbacklength',\n",
    " 'waistbreadth',\n",
    " 'waistcircumference',\n",
    " 'waistdepth',\n",
    " 'waistfrontlengthsitting',\n",
    " 'waistheightomphalion',\n",
    " 'wristcircumference',\n",
    " 'wristheight',\n",
    " 'weight_kg',\n",
    " 'stature_m',\n",
    " 'BMI']\n",
    "#Select the columns described in mask\n",
    "ansur_df=ansur_df.loc[:, mask]\n",
    "#Show the final dimensions of the ansur_df dataset\n",
    "print(ansur_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Involucre en el Pipeline un escalar a PCA seleccionado 80% de la varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe a scaler to PCA selecting 80% of the variance\n",
    "pipe = ____([('scaler', ____),\n",
    "        \t\t ('reducer', ____)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste el Pipe a la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipe to the data\n",
    "____\n",
    "\n",
    "print('{} components selected'.format(len(pipe.steps[1][1].components_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incremente la proporción de varianza al 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let PCA select 90% of the variance\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=____))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "print('{} components selected'.format(len(pipe.steps[1][1].components_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta<br>\n",
    "¿Cuántas características adicionales necesita para explicar el 90% en lugar del 80% de la varianza?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Seleccionando el número de componentes\n",
    "\n",
    "Ahora tomará una decisión más informada sobre el número de componentes principales para reducir sus datos. Usando  la técnica de \"codo en la gráfica\". Una última vez, trabajará en el conjunto de datos numérico ANSUR femenino, que ha sido  precargado como ansur_df.\n",
    "\n",
    "Todos los paquetes y las clases pertinentes han sido pre-cargadas para usted ( Pipeline(), StandardScaler(), PCA())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un pipeline con un escalar y un PCA seleccionando 10 componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline a scaler and PCA selecting 10 components\n",
    "pipe = ____([('scaler', ____),\n",
    "        \t\t ('reducer', ____)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste el pipeline a la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipe to the data\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafique la razón de varianza explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance ratio\n",
    "plt.plot(pipe.steps[1][1].____)\n",
    "\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta<br>\n",
    "¿A cuántos componentes puede reducir el conjunto de datos sin comprometer demasiado la varianza explicada?<br>\n",
    "(Tenga en cuenta que el eje x está indexado a cero)<br>\n",
    "Respuestas posibles<br>\n",
    "1<br>\n",
    "2<br>\n",
    "3<br>\n",
    "4<br> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
